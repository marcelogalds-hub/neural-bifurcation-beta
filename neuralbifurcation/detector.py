"""
Neural Bifurcation Framework - Multi-Objective Regime Detector
===============================================================

Este m√≥dulo implementa o detector de regime multi-objetivo, o componente central
do Neural Bifurcation Framework. Ele monitora o treinamento de redes neurais e
decide quando parar baseado em objetivos como accuracy, robustez, custo ou equil√≠brio.

O detector funciona como um callback do Keras/TensorFlow que:
- Monitora m√©tricas de treino e valida√ß√£o em cada √©poca
- Calcula indicadores de generaliza√ß√£o (At) e robustez (Rt)
- Classifica o estado atual do aprendizado
- Decide quando parar baseado no objetivo escolhido
- Restaura os melhores pesos automaticamente

Classes principais:
    MultiObjectiveRegimeDetector: Callback principal para detec√ß√£o de regime

Exemplo de uso b√°sico:
    >>> from neuralbifurcation import MultiObjectiveRegimeDetector
    >>> 
    >>> # Criar detector com objetivo de robustez
    >>> detector = MultiObjectiveRegimeDetector(
    ...     objective='robustness',
    ...     patience=10,
    ...     verbose=1
    ... )
    >>> 
    >>> # Usar no treinamento
    >>> model.fit(
    ...     X_train, y_train,
    ...     validation_data=(X_val, y_val),
    ...     epochs=100,
    ...     callbacks=[detector]
    ... )
    >>> 
    >>> # Obter relat√≥rio
    >>> report = detector.get_multi_objective_report()

Autor: Marcelo Galdino de Souza
Data: 31 de outubro de 2025
Licen√ßa: MIT
"""

import numpy as np
from tensorflow import keras
from typing import Dict, List, Optional, Any, Tuple
import warnings

from .objectives import ObjectiveLibrary, ObjectiveConfig
from .states import StateClassifier, SystemState

warnings.filterwarnings('ignore')


class MultiObjectiveRegimeDetector(keras.callbacks.Callback):
    """
    Detector de regime com suporte a m√∫ltiplos objetivos de otimiza√ß√£o.
    
    Este callback monitora o treinamento de redes neurais e implementa estrat√©gias
    de parada inteligente baseadas em diferentes objetivos:
    - 'accuracy': Maximiza acur√°cia de valida√ß√£o
    - 'robustness': Maximiza generaliza√ß√£o (At) e estabilidade (Rt)
    - 'cost': Minimiza custo computacional mantendo qualidade
    - 'balanced': Equilibra todos os objetivos
    - 'discovery': Explora mais √©pocas para encontrar melhores solu√ß√µes
    
    O detector calcula automaticamente:
    - At (Autonomia): Raz√£o val_acc/train_acc, indica generaliza√ß√£o
    - Rt (Robustez): Estabilidade temporal de At
    - ROI: Retorno sobre investimento (melhoria/custo)
    - Estado do aprendizado: classifica√ß√£o do regime atual
    
    Attributes:
        objective (str): Objetivo de otimiza√ß√£o selecionado
        patience (int): √âpocas sem melhora antes de parar
        cost_per_epoch (float): Custo estimado por √©poca em USD
        verbose (int): N√≠vel de verbosidade (0=silencioso, 1=normal, 2=debug)
        config (ObjectiveConfig): Configura√ß√£o do objetivo selecionado
        classifier (StateClassifier): Classificador de estados de aprendizado
        history (Dict): Hist√≥rico completo de m√©tricas por √©poca
        best_epoch (int): √âpoca com melhor score
        best_score (float): Melhor score alcan√ßado
        best_weights (List): Pesos do modelo na melhor √©poca
        total_cost (float): Custo acumulado do treinamento
    
    Example:
        >>> # Uso b√°sico com objetivo balanceado
        >>> detector = MultiObjectiveRegimeDetector(
        ...     objective='balanced',
        ...     patience=10,
        ...     verbose=1
        ... )
        >>> model.fit(X, y, callbacks=[detector], epochs=100)
        
        >>> # Uso focado em robustez com custo customizado
        >>> detector = MultiObjectiveRegimeDetector(
        ...     objective='robustness',
        ...     patience=15,
        ...     cost_per_epoch=5.0,
        ...     verbose=2
        ... )
        >>> model.fit(X, y, callbacks=[detector], epochs=100)
        
        >>> # An√°lise p√≥s-treinamento
        >>> report = detector.get_multi_objective_report()
        >>> print(f"Melhor √©poca: {report['recommended_epoch']}")
        >>> print(f"Custo total: ${report['total_cost']:.2f}")
        
        >>> # Visualiza√ß√£o
        >>> detector.plot_comparison('results.png')
    
    Note:
        - O detector SEMPRE restaura os melhores pesos ao final do treinamento
        - At < 0.8 indica overfitting severo (val 20% abaixo de train)
        - At > 0.95 indica generaliza√ß√£o excelente
        - Rt pr√≥ximo de 1.0 indica alta estabilidade
        - O baseline √© calibrado ap√≥s 5 √©pocas iniciais
    
    Warning:
        Este c√≥digo foi extensivamente testado e validado em aplica√ß√µes cr√≠ticas.
        Modifica√ß√µes na l√≥gica de c√°lculo ou decis√£o podem quebrar funcionalidades.
    """
    
    def __init__(
        self, 
        objective: str = 'balanced',
        patience: int = 10,
        cost_per_epoch: float = 2.0,
        verbose: int = 1
    ) -> None:
        """
        Inicializa o detector multi-objetivo.
        
        Args:
            objective: Objetivo de otimiza√ß√£o. Op√ß√µes:
                - 'accuracy': Foca em maximizar acur√°cia de valida√ß√£o
                - 'robustness': Foca em generaliza√ß√£o e estabilidade
                - 'cost': Minimiza custo mantendo qualidade adequada
                - 'balanced': Equilibra accuracy, robustez e custo
                - 'discovery': Explora mais para encontrar melhores solu√ß√µes
                Default: 'balanced'
            patience: N√∫mero de √©pocas sem melhora no score antes de parar.
                Valores t√≠picos: 5-20 dependendo da complexidade do problema.
                Default: 10
            cost_per_epoch: Custo estimado por √©poca em USD, usado para
                c√°lculo de ROI e otimiza√ß√£o de custo. Considere tempo de GPU,
                energia e recursos computacionais.
                Default: 2.0
            verbose: N√≠vel de informa√ß√£o durante treinamento:
                - 0: Silencioso, sem output
                - 1: Normal, mostra progresso e alertas
                - 2: Debug, mostra informa√ß√µes detalhadas
                Default: 1
        
        Raises:
            ValueError: Se objective n√£o for uma op√ß√£o v√°lida
            ValueError: Se patience < 1
            ValueError: Se cost_per_epoch < 0
            ValueError: Se verbose n√£o for 0, 1 ou 2
        
        Example:
            >>> # Configura√ß√£o para produ√ß√£o (foco em custo)
            >>> detector = MultiObjectiveRegimeDetector(
            ...     objective='cost',
            ...     patience=8,
            ...     cost_per_epoch=3.50,
            ...     verbose=1
            ... )
            
            >>> # Configura√ß√£o para pesquisa (foco em descoberta)
            >>> detector = MultiObjectiveRegimeDetector(
            ...     objective='discovery',
            ...     patience=20,
            ...     cost_per_epoch=1.0,
            ...     verbose=2
            ... )
            
            >>> # Configura√ß√£o para aplica√ß√µes cr√≠ticas (foco em robustez)
            >>> detector = MultiObjectiveRegimeDetector(
            ...     objective='robustness',
            ...     patience=15,
            ...     cost_per_epoch=5.0,
            ...     verbose=1
            ... )
        """
        super().__init__()
        
        self.objective = objective
        self.patience = patience
        self.cost_per_epoch = cost_per_epoch
        self.verbose = verbose
        
        # Carregar configura√ß√£o do objetivo selecionado
        # Isso valida automaticamente se o objetivo existe
        self.config = ObjectiveLibrary.get_config(objective)
        
        # Inicializar classificador de estados com janela de 5 √©pocas
        self.classifier = StateClassifier(window_size=5)
        
        # Inicializar estado interno
        self.reset()
    
    def reset(self) -> None:
        """
        Reseta o estado interno do detector.
        
        Limpa todo o hist√≥rico e reinicializa vari√°veis de controle.
        √ötil se o mesmo detector for reutilizado em m√∫ltiplos treinos.
        
        Note:
            Este m√©todo √© chamado automaticamente em on_train_begin().
            Raramente precisa ser chamado manualmente.
        """
        # Hist√≥rico completo de m√©tricas por √©poca
        self.history = {
            'epoch': [],           # N√∫mero da √©poca
            'train_acc': [],       # Acur√°cia de treino
            'val_acc': [],         # Acur√°cia de valida√ß√£o
            'train_loss': [],      # Loss de treino
            'val_loss': [],        # Loss de valida√ß√£o
            'At': [],              # Autonomia (val_acc/train_acc)
            'Rt': [],              # Robustez (estabilidade temporal)
            'theta': [],           # √Çngulo do gap (arctan)
            'gap': [],             # Gap = train_acc - val_acc
            'roi': [],             # Return on Investment
            'state': [],           # Estado classificado
            'score': []            # Score do objetivo atual
        }
        
        # Controle do melhor modelo
        self.best_epoch = 0
        self.best_score = -float('inf')
        self.best_weights = None
        self.epochs_no_improve = 0
        
        # Calibra√ß√£o de baseline (primeiras 5 √©pocas)
        self.baseline_calibrated = False
        self.At_baseline = None
        
        # Controle de custo
        self.total_cost = 0.0
    
    def on_train_begin(self, logs: Optional[Dict] = None) -> None:
        """
        Callback chamado no in√≠cio do treinamento.
        
        Reseta o estado interno e exibe informa√ß√µes sobre o objetivo selecionado
        se verbose >= 1.
        
        Args:
            logs: Dicion√°rio de logs do Keras (geralmente vazio neste callback)
        
        Note:
            Este m√©todo √© chamado automaticamente pelo Keras antes da primeira √©poca.
        """
        self.reset()
        
        if self.verbose >= 1:
            print("\n" + "="*80)
            print(f"üéØ Multi-Objective Regime Detector V4.0")
            print("="*80)
            print(f"Objetivo selecionado: {self.config.name}")
            print(f"Descri√ß√£o: {self.config.description}")
            print(f"M√©trica prim√°ria: {self.config.primary_metric}")
            print("="*80)
            print()
    
    def on_epoch_end(self, epoch: int, logs: Optional[Dict] = None) -> None:
        """
        Callback chamado ao final de cada √©poca.
        
        Este √© o m√©todo principal do detector. A cada √©poca ele:
        1. Coleta m√©tricas de treino e valida√ß√£o
        2. Calcula indicadores derivados (At, Rt, ROI, etc.)
        3. Calibra baseline nas primeiras 5 √©pocas
        4. Classifica o estado atual do aprendizado
        5. Calcula score baseado no objetivo
        6. Decide se deve continuar ou parar
        7. Exibe informa√ß√µes e alertas
        
        Args:
            epoch: N√∫mero da √©poca atual (0-indexed)
            logs: Dicion√°rio com m√©tricas do Keras. Esperado conter:
                - 'accuracy': Acur√°cia de treino
                - 'val_accuracy': Acur√°cia de valida√ß√£o
                - 'loss': Loss de treino
                - 'val_loss': Loss de valida√ß√£o
        
        Note:
            - At = val_acc / train_acc: mede generaliza√ß√£o
              * At < 0.8: overfitting severo
              * At 0.8-0.9: overfitting moderado
              * At 0.9-1.0: generaliza√ß√£o boa
              * At > 1.0: poss√≠vel underfitting ou dados ruins
            
            - Rt: estabilidade de At nas √∫ltimas √©pocas
              * Rt pr√≥ximo de 1.0: muito est√°vel
              * Rt pr√≥ximo de 0.0: inst√°vel
            
            - ROI: melhoria / custo nas √∫ltimas 5 √©pocas
              * ROI alto: bom retorno sobre investimento
              * ROI baixo: pouco progresso por custo
            
            - O baseline √© calibrado na √©poca 4 (quinta √©poca)
            - Decis√£o de parada considera estado + patience + score
        
        Warning:
            A l√≥gica de c√°lculo de At, Rt e decis√£o de parada foi extensivamente
            testada. Modifica√ß√µes podem quebrar o comportamento esperado.
        """
        logs = logs or {}
        
        # ====================================
        # 1. COLETAR M√âTRICAS B√ÅSICAS
        # ====================================
        train_acc = logs.get('accuracy', 0)
        val_acc = logs.get('val_accuracy', 0)
        train_loss = logs.get('loss', 0)
        val_loss = logs.get('val_loss', 0)
        
        # ====================================
        # 2. CALCULAR M√âTRICAS DERIVADAS
        # ====================================
        
        # At (Autonomia): val_acc / train_acc
        # Mede qu√£o bem o modelo generaliza
        # Epsilon evita divis√£o por zero
        At = val_acc / (train_acc + 1e-6)
        
        # Gap: diferen√ßa entre treino e valida√ß√£o
        # Gap alto indica overfitting
        gap = train_acc - val_acc
        
        # Theta: √¢ngulo do gap (para visualiza√ß√£o geom√©trica)
        # Usado em an√°lises topol√≥gicas (n√£o na decis√£o)
        theta = np.arctan(max(gap, 0))
        
        # Rt (Robustez): estabilidade temporal de At
        # Simplificado por enquanto, vers√£o completa requer ativa√ß√µes
        Rt = self._compute_rt_simple(epoch)
        
        # ROI: Return on Investment
        # Melhoria obtida dividida pelo custo nas √∫ltimas 5 √©pocas
        if len(self.history['val_acc']) >= 5:
            improvement = val_acc - self.history['val_acc'][-5]
            cost = 5 * self.cost_per_epoch
            roi = (improvement * 100) / cost if cost > 0 else 0
        else:
            roi = 0
        
        # Atualizar custo acumulado
        self.total_cost += self.cost_per_epoch
        
        # ====================================
        # 3. SALVAR NO HIST√ìRICO
        # ====================================
        self.history['epoch'].append(epoch)
        self.history['train_acc'].append(train_acc)
        self.history['val_acc'].append(val_acc)
        self.history['train_loss'].append(train_loss)
        self.history['val_loss'].append(val_loss)
        self.history['At'].append(At)
        self.history['Rt'].append(Rt)
        self.history['theta'].append(theta)
        self.history['gap'].append(gap)
        self.history['roi'].append(roi)
        
        # ====================================
        # 4. CALIBRAR BASELINE (√©poca 4 = quinta √©poca)
        # ====================================
        # O baseline √© a m√©dia de At nas primeiras 5 √©pocas
        # Usado como refer√™ncia para detectar desvios
        if epoch == 4 and not self.baseline_calibrated:
            self.At_baseline = np.mean(self.history['At'])
            self.baseline_calibrated = True
            
            if self.verbose >= 1:
                print(f"\nüìä Baseline calibrado: At = {self.At_baseline:.3f}\n")
        
        # ====================================
        # 5. CLASSIFICAR ESTADO DO APRENDIZADO
        # ====================================
        # Usa StateClassifier para identificar regime atual:
        # inicializa√ß√£o, aprendizado_r√°pido, overfitting, plateau, etc.
        if self.baseline_calibrated:
            state = self.classifier.classify(self.history, epoch)
            self.history['state'].append(state.value)
        else:
            # Nas primeiras 5 √©pocas, sempre est√° inicializando
            state = SystemState.INITIALIZATION
            self.history['state'].append(state.value)
        
        # ====================================
        # 6. AVALIAR BASEADO NO OBJETIVO
        # ====================================
        # Cada objetivo tem uma fun√ß√£o de decis√£o pr√≥pria
        # que pondera as m√©tricas de forma diferente
        metrics = {
            'val_acc': val_acc,
            'At': At,
            'Rt': Rt,
            'roi': roi
        }
        
        # Calcular score usando fun√ß√£o do objetivo
        score = self.config.decision_function(metrics)
        self.history['score'].append(score)
        
        # Verificar se houve melhora
        improved = score > self.best_score
        
        if improved:
            # Novo melhor score! Atualizar e salvar pesos
            self.best_score = score
            self.best_epoch = epoch
            self.best_weights = self.model.get_weights()
            self.epochs_no_improve = 0
        else:
            # Sem melhora, incrementar contador de patience
            self.epochs_no_improve += 1
        
        # ====================================
        # 7. OUTPUT E ALERTAS
        # ====================================
        if self.verbose >= 1:
            state_info = self.classifier.get_state_info(state)
            
            # Linha principal com m√©tricas
            print(f"{state_info['emoji']} Epoch {epoch+1:3d} | "
                  f"Val: {val_acc:.3f} | "
                  f"At: {At:.3f} | "
                  f"Rt: {Rt:.2f} | "
                  f"ROI: ${roi:.2f}/pt | "
                  f"Score: {score:.3f} | "
                  f"{state.value}")
            
            # Alertas e sugest√µes para estados problem√°ticos
            if state_info['severity'] in ['warning', 'critical']:
                print(f"   {state_info['description']}")
                
                if 'suggestions' in state_info:
                    print(f"   Sugest√µes: {', '.join(state_info['suggestions'])}")
        
        # ====================================
        # 8. DECIS√ÉO DE PARADA
        # ====================================
        should_stop = False
        stop_reason = None
        
        # Crit√©rio 1: Estado cr√≠tico que exige parada imediata
        # Ex: overfitting severo, instabilidade cr√≠tica
        state_info = self.classifier.get_state_info(state)
        if state_info['action'] == 'stop' and state_info['urgency'] in ['high', 'critical']:
            should_stop = True
            stop_reason = f"{state_info['description']}"
        
        # Crit√©rio 2: Patience esgotada
        # Muitas √©pocas sem melhora no score
        elif self.epochs_no_improve >= self.patience:
            should_stop = True
            stop_reason = f"Sem melhora h√° {self.epochs_no_improve} epochs"
        
        # Executar parada se necess√°rio
        if should_stop:
            if self.verbose >= 1:
                print(f"\n{'='*80}")
                print(f"üõë PARANDO TREINO")
                print(f"{'='*80}")
                print(f"Motivo: {stop_reason}")
                print(f"Melhor epoch: {self.best_epoch + 1}")
                print(f"Score: {self.best_score:.3f}")
                print(f"{'='*80}\n")
            
            self.model.stop_training = True
    
    def on_train_end(self, logs: Optional[Dict] = None) -> None:
        """
        Callback chamado ao final do treinamento.
        
        Restaura os pesos da melhor √©poca (baseado no score do objetivo)
        e exibe relat√≥rio final se verbose >= 1.
        
        Args:
            logs: Dicion√°rio de logs do Keras (geralmente vazio neste callback)
        
        Note:
            A restaura√ß√£o de pesos √© CR√çTICA: garante que o modelo final
            seja o melhor encontrado durante o treinamento, n√£o o √∫ltimo.
        """
        # Restaurar melhor modelo encontrado
        if self.best_weights is not None:
            if self.verbose >= 1:
                print(f"\n‚úÖ Restaurando modelo do epoch {self.best_epoch + 1}")
            self.model.set_weights(self.best_weights)
        
        # Mostrar relat√≥rio final comparativo
        if self.verbose >= 1:
            self._print_final_report()
    
    def _compute_rt_simple(self, epoch: int) -> float:
        """
        Calcula Rt (Robustez) de forma simplificada.
        
        Esta √© uma vers√£o placeholder baseada na estabilidade de At.
        A vers√£o completa requer an√°lise das ativa√ß√µes da rede (n√£o implementada).
        
        Args:
            epoch: √âpoca atual (usado para verificar hist√≥rico dispon√≠vel)
        
        Returns:
            Rt normalizado em [0, 0.5]:
            - Valores altos (~0.5): At muito est√°vel
            - Valores baixos (~0.1): At inst√°vel
        
        Note:
            - Usa √∫ltimas 3 √©pocas para calcular estabilidade
            - Rt = (1 - std(At)) * 0.5 para normalizar
            - Retorna 0.1 se n√£o h√° hist√≥rico suficiente
            - Vers√£o futura analisar√° topologia das ativa√ß√µes
        """
        # Precisa de pelo menos 3 √©pocas para calcular estabilidade
        if len(self.history['At']) < 3:
            return 0.1
        
        # Pegar √∫ltimas 3 medi√ß√µes de At
        At_recent = self.history['At'][-3:]
        
        # Calcular estabilidade: quanto menor o desvio, maior a estabilidade
        # std alto = inst√°vel, std baixo = est√°vel
        # Limitado a 1.0 para evitar valores negativos
        stability = 1.0 - min(np.std(At_recent), 1.0)
        
        # Normalizar para [0, 0.5]
        return stability * 0.5
    
    def _print_final_report(self) -> None:
        """
        Imprime relat√≥rio final com an√°lise multi-objetivo.
        
        Compara o resultado do objetivo selecionado com todos os outros
        objetivos poss√≠veis, mostrando trade-offs e recomenda√ß√µes.
        
        Note:
            - S√≥ √© chamado se verbose >= 1
            - Usa ObjectiveComparator para an√°lise comparativa
            - Mostra melhor √©poca e m√©tricas para cada objetivo
            - Destaca trade-offs significativos (>2% acc, >0.1 At, >$10)
        """
        print("\n" + "="*80)
        print("üìä RELAT√ìRIO FINAL - AN√ÅLISE MULTI-OBJETIVO")
        print("="*80)
        print()
        
        # Comparar todos os objetivos poss√≠veis
        # comparison = ObjectiveComparator.compare_all(self.history)
        
        # Tabela comparativa
        print(f"{'Objetivo':<20} | {'Melhor Epoch':<12} | {'Val Acc':<8} | {'At':<6} | {'ROI':<8}")
        print("-" * 80)
        
        for obj_name, result in comparison.items():
            metrics = result['metrics_at_best']
            print(f"{obj_name:<20} | "
                  f"{result['best_epoch']+1:<12} | "
                  f"{metrics['val_acc']:.3f}    | "
                  f"{metrics['At']:.2f} | "
                  f"${metrics['roi']:>6.2f}")
        
        print("="*80)
        print()
        
        # Recomenda√ß√£o para o objetivo selecionado
        selected = comparison[self.objective]
        print(f"üéØ RECOMENDA√á√ÉO (objetivo: {self.objective})")
        print(f"   Epoch: {selected['best_epoch'] + 1}")
        print(f"   Val Accuracy: {selected['metrics_at_best']['val_acc']:.3f}")
        print(f"   Autonomia (At): {selected['metrics_at_best']['At']:.3f}")
        print(f"   Custo total: ${self.total_cost:.2f}")
        print()
        
        # An√°lise de trade-offs
        print("üí° TRADE-OFFS:")
        self._print_tradeoffs(comparison)
        
        print("="*80)
        print()
    
    def _print_tradeoffs(self, comparison: Dict) -> None:
        """
        Imprime an√°lise de trade-offs entre objetivos.
        
        Compara o objetivo selecionado com os demais, mostrando diferen√ßas
        significativas em accuracy, At e custo.
        
        Args:
            comparison: Resultado de ObjectiveComparator.compare_all()
        
        Note:
            - S√≥ mostra diferen√ßas > 2% em accuracy
            - S√≥ mostra diferen√ßas > 0.1 em At
            - S√≥ mostra diferen√ßas > $10 em custo
            - Sinal + indica que o outro objetivo √© melhor naquela m√©trica
        """
        selected = comparison[self.objective]
        selected_metrics = selected['metrics_at_best']
        
        for obj_name, result in comparison.items():
            if obj_name == self.objective:
                continue
            
            metrics = result['metrics_at_best']
            
            # Calcular diferen√ßas
            acc_diff = metrics['val_acc'] - selected_metrics['val_acc']
            At_diff = metrics['At'] - selected_metrics['At']
            epoch_diff = result['best_epoch'] - selected['best_epoch']
            cost_diff = epoch_diff * self.cost_per_epoch
            
            # S√≥ mostrar se houver diferen√ßas significativas
            if abs(acc_diff) > 0.02 or abs(At_diff) > 0.1 or abs(cost_diff) > 10:
                print(f"\n   vs {obj_name}:")
                if acc_diff != 0:
                    sign = "+" if acc_diff > 0 else ""
                    print(f"      Accuracy: {sign}{acc_diff*100:.1f}%")
                if At_diff != 0:
                    sign = "+" if At_diff > 0 else ""
                    print(f"      At (robustez): {sign}{At_diff:.2f}")
                if cost_diff != 0:
                    sign = "+" if cost_diff > 0 else ""
                    print(f"      Custo: {sign}${cost_diff:.2f}")
    
    def get_multi_objective_report(self) -> Dict[str, Any]:
        """
        Retorna relat√≥rio completo estruturado para an√°lise program√°tica.
        
        Returns:
            Dicion√°rio contendo:
            - 'selected_objective': Nome do objetivo usado
            - 'recommended_epoch': Melhor √©poca (1-indexed)
            - 'total_epochs': Total de √©pocas treinadas
            - 'total_cost': Custo total em USD
            - 'all_objectives': Compara√ß√£o de todos objetivos poss√≠veis
            - 'history': Hist√≥rico completo de todas m√©tricas
            - 'config': Configura√ß√£o do objetivo selecionado
        
        Example:
            >>> report = detector.get_multi_objective_report()
            >>> print(f"Melhor √©poca: {report['recommended_epoch']}")
            >>> print(f"Custo: ${report['total_cost']:.2f}")
            >>> 
            >>> # Acessar compara√ß√£o entre objetivos
            >>> for obj, data in report['all_objectives'].items():
            ...     print(f"{obj}: epoch {data['best_epoch']+1}")
            >>> 
            >>> # Plotar hist√≥rico customizado
            >>> import matplotlib.pyplot as plt
            >>> plt.plot(report['history']['val_acc'])
            >>> plt.show()
        
        Note:
            Este m√©todo pode ser chamado a qualquer momento, mas √© mais √∫til
            ap√≥s o treinamento terminar.
        """
        # Comparar todos os objetivos
        comparison = ObjectiveComparator.compare_all(self.history)
        
        return {
            'selected_objective': self.objective,
            'recommended_epoch': self.best_epoch + 1,
            'total_epochs': len(self.history['epoch']),
            'total_cost': self.total_cost,
            'all_objectives': comparison,
            'history': self.history,
            'config': {
                'name': self.config.name,
                'description': self.config.description,
                'tolerance': self.config.tolerance
            }
        }
    
    def plot_comparison(self, save_path: str = 'multi_objective_comparison.png') -> Any:
        """
        Plota compara√ß√£o visual entre objetivos e salva figura.
        
        Cria uma figura 2x2 com:
        - Plot 1: Val Accuracy ao longo do tempo
        - Plot 2: At (Autonomia/Generaliza√ß√£o) ao longo do tempo
        - Plot 3: ROI ao longo do tempo
        - Plot 4: Estados de aprendizado coloridos
        
        Args:
            save_path: Caminho onde salvar a figura PNG.
                Default: 'multi_objective_comparison.png'
        
        Returns:
            Objeto Figure do matplotlib
        
        Example:
            >>> # Salvar com nome padr√£o
            >>> detector.plot_comparison()
            üìä Gr√°fico salvo: multi_objective_comparison.png
            
            >>> # Salvar em local espec√≠fico
            >>> detector.plot_comparison('results/experiment_01.png')
            üìä Gr√°fico salvo: results/experiment_01.png
            
            >>> # Obter figura para manipula√ß√£o
            >>> fig = detector.plot_comparison()
            >>> fig.suptitle('Meu Experimento')
            >>> fig.savefig('custom.png')
        
        Note:
            - Requer matplotlib instalado
            - Marca a melhor √©poca (linha vermelha tracejada)
            - Estados s√£o coloridos por severidade
            - Resolu√ß√£o: 150 DPI
        """
        import matplotlib.pyplot as plt
        
        # Criar figura 2x2
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Multi-Objective Training Analysis', fontsize=16, fontweight='bold')
        
        # Epochs 1-indexed para visualiza√ß√£o
        epochs = np.array(self.history['epoch']) + 1
        
        # ====================================
        # Plot 1: Val Accuracy
        # ====================================
        ax = axes[0, 0]
        ax.plot(epochs, self.history['val_acc'], 'b-', linewidth=2, label='Val Accuracy')
        ax.axvline(self.best_epoch + 1, color='r', linestyle='--', label=f'Best ({self.objective})')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Validation Accuracy')
        ax.set_title('Validation Accuracy Over Time')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # ====================================
        # Plot 2: At (Autonomia)
        # ====================================
        ax = axes[0, 1]
        ax.plot(epochs, self.history['At'], 'g-', linewidth=2, label='At')
        # Linha do baseline se calibrado
        if self.At_baseline:
            ax.axhline(self.At_baseline, color='gray', linestyle=':', label='Baseline')
        ax.axvline(self.best_epoch + 1, color='r', linestyle='--')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Autonomia (At)')
        ax.set_title('Generalization (At) Over Time')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # ====================================
        # Plot 3: ROI
        # ====================================
        ax = axes[1, 0]
        ax.plot(epochs, self.history['roi'], 'orange', linewidth=2, label='ROI')
        ax.axvline(self.best_epoch + 1, color='r', linestyle='--')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('ROI ($/point)')
        ax.set_title('Return on Investment Over Time')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # ====================================
        # Plot 4: States (Estados coloridos)
        # ====================================
        ax = axes[1, 1]
        
        # Mapeamento estado -> cor
        state_colors = {
            'inicializacao': 'gray',
            'aprendizado_rapido': 'green',
            'aprendizado_saudavel': 'lightgreen',
            'overfitting_inicial': 'yellow',
            'overfitting_severo': 'red',
            'plateau': 'orange',
            'underfitting': 'blue',
            'instabilidade': 'purple'
        }
        
        # Plotar cada √©poca com cor do estado
        for i, state in enumerate(self.history['state']):
            color = state_colors.get(state, 'gray')
            ax.scatter(epochs[i], self.history['val_acc'][i], c=color, s=50, alpha=0.7)
        
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Val Accuracy')
        ax.set_title('Training States')
        ax.grid(True, alpha=0.3)
        
        # Legenda de cores dos estados
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor=color, label=state.replace('_', ' ').title()) 
                          for state, color in state_colors.items()]
        ax.legend(handles=legend_elements, loc='best', fontsize=8)
        
        # Salvar figura
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"\nüìä Gr√°fico salvo: {save_path}")
        
        return fig
